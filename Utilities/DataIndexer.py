import numpy as np
import pandas as pd
import argparse
from IPython.display import display

def main():

    parser = argparse.ArgumentParser(description="Imports Indexer for Blue Sentry")
    parser.add_argument("--output_file", help="File that the imports are indexed to", required=True)
    parser.add_argument("--input_csv", help="The csv file with all files indexed by the fileindexer", required=True)
    parser.add_argument("--relevance", help="How many times an import needs to be seen before writing to the output "
                                            "file. This can help avoid overfitting by not using underused imports "
                                            "to train later models", required=True)
    parser.add_argument("--to_index", help="What you want to index (Currently string or imports)", default="imports")
    args = parser.parse_args()

    data = None

    imports_dict = {}
    count = 0
    for chunk in pd.read_csv(args.input_csv, chunksize=5000, error_bad_lines=False):

        # display(data.describe)

        relevance = int(args.relevance)

        for index, row in chunk.iterrows():
            imports_data = row[args.to_index].split()

            for item in imports_data:
                item = item.strip()
                item = item.replace('[', '')
                item = item.replace(']', '')
                item = item.replace('\'', '')
                item = item.replace(',', '')
                if item not in imports_dict:
                    imports_dict[item] = 1
                else:
                    imports_dict[item] += 1
            count += 1

            if count % 10000 == 0:
                print count
        # print imports_dict.keys()

    imports_file = open(args.output_file, 'w+')
    count = 0
    for import_name in imports_dict:
        if imports_dict[import_name] > relevance:
            imports_file.write(import_name + "\n")
            count += 1

    imports_file.close()
    print count

if __name__ == "__main__":

    main()
