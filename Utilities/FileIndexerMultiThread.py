import glob2
import csv
import hashlib
import os
import pefile
import entropy
import argparse
import magic
import threading
import time
import string

field_names = ['sha256', 'strings', 'imports', 'size', 'signed', 'good_file', 'entropy']
csv_file = None
writer = None
file_lock = threading.Lock()
hashing_lock = threading.Lock()
count = 0

def get_file_size(FilePath):
    return os.path.getsize(FilePath)


def get_file_signatures(FilePath):
 
    try:
        pe = pefile.PE(FilePath)
 
        address = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_SECURITY']].VirtualAddress
        size = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_SECURITY']].Size
        if address == 0:
            return 0
        return 1
    except:
        return -1


def get_md5_of_file(FilePath):
    return hashlib.md5(FilePath).hexdigest()


def get_sha256_of_file(FilePath):
    return hashlib.sha256(FilePath).hexdigest()


def get_filename_from_path(FilePath):
    FileSplitPath = FilePath.split("\\")
    return FileSplitPath[-1]


def get_file_path(FilePath):
    FileSplitPath = FilePath.split("\\")
    seq = "\\"
    return seq.join(FileSplitPath[0:len(FileSplitPath) - 1])


def get_import_table_entries(FilePath):
    imports = []

    try:
        pe = pefile.PE(FilePath)

        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            # print entry.dll
            for imp in entry.imports:
                imports.append(imp.name)
    except:
        # print "Error: ", filepath
        imports.append("Error")
    return imports


def get_strings(filename, min_length):
    file_data = open(filename, 'rb')

    strings_list = []

    current = ''
    for character in file_data.read():
        if character in string.printable:
            current += character
        else:
            if len(current) > min_length:
                strings_list.append(current)

            current = ''

    return strings_list


def get_entropy_of_file(FilePath):
    file_read = open(FilePath, 'r')
    file_data = file_read.read()

    entropy_file = entropy.shannon_entropy(file_data)
    file_read.close()
    return entropy_file


def get_type_of_file(FilePath):
    magic_type = magic.from_file(FilePath)
    
    if 'DLL' in magic_type:
        return 0
    else:
        return 1


def is_pefile(FilePath):
    is_PE = 0
    file_handle = open(FilePath, "rb")

    if file_handle.read(2) == "MZ":
        is_PE = 1
    file_handle.close()

    return is_PE


def get_stats_info(filename, all_pe, sha256_dict, good_file):

    global writer
    global count
    if all_pe == 1 or is_pefile(filename):

            sha256 = get_sha256_of_file(filename)

            duplicate = False
            if sha256 in sha256_dict:
                duplicate = True

            if duplicate is False:
                signed = get_file_signatures(filename)
                entropy = get_entropy_of_file(filename)
                size = get_file_size(filename)
                imports = get_import_table_entries(filename)
                strings = get_strings(filename, 8)
                file_lock.acquire()
                writer.writerow(
                    {'sha256': sha256, 'strings': strings, 'imports': imports, 'size': size, 'signed': signed, 'good_file': good_file,
                     'entropy': entropy})
                csv_file.flush()
                sha256_dict[sha256] = 1
                count += 1
                file_lock.release()
                if count % 100 == 0:
                    print "Got through: ", count


def write_stats_to_csv(csv_file, path, good_file, all_pe):
    global count
    files = glob2.glob(path)

    sha256_dict = {}
    start_time = time.time()
    duplicates = 0
    for filename in files:
        while threading.activeCount() > 100:
            pass
        arg = [filename, all_pe, sha256_dict, good_file]
        threading.Thread(target=get_stats_info, args=arg).start()
        print count
        if count > 100:
            print "Breaking"
            break

    print "Thread got to 500!"
    # activecount() also includes the parent thread creating threads
    while threading.activeCount() > 1:
        print threading.activeCount()
        time.sleep(1)

    end_time = time.time()

    total_time = end_time - start_time
    print "Total Time:", str(total_time)
    print "\nDone: %s" % path
    print "Duplicates: %d\n" % duplicates


def main():
    parser = argparse.ArgumentParser(description="File Indexer for Blue Sentry")
    parser.add_argument("--output_file", help="File that the data is output to in csv form", required=True)
    parser.add_argument("--input_directory", help="Directory for which the parser will run through for "
                                                  "executables and DLLs", required=True)
    parser.add_argument("--good_file", help="An indication whether or not the files in the input directory are known "
                                            "good or bad. 0 for bad, 1 for good", required=True)
    parser.add_argument("--all_pe", help="Indication whether all the files in the current directory are known to be PE files", default=0)

    args = parser.parse_args()

    #csv_file = open('data_working.csv', 'a+')
    write_header = True
    if os.path.isfile(args.output_file):
        print "File!"
        write_header = False
    global csv_file
    csv_file = open(args.output_file, 'a+')
    global writer
    writer = csv.DictWriter(csv_file, field_names)
    if write_header:
        writer.writeheader()

    write_stats_to_csv(csv_file, args.input_directory, int(args.good_file), int(args.all_pe))
    #write_stats_to_csv(csv_file, '/home/processing/Desktop/ML/ExeFolder/*', 1)
    #write_stats_to_csv(csv_file, '/home/processing/Desktop/ML/KnownBad/VirusShare_PE32_Win_20150129/*', 0)

    csv_file.close()

if __name__ == "__main__":
    main()
