import numpy as np
import pandas as pd
import joblib
import argparse
from IPython.display import display
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.neural_network import MLPClassifier
import random
import tensorflow as tf
from tensorflow.contrib.layers.python.layers import encoders

CATEGORICAL_COLUMNS = ["imports"]
LABEL_COLUMN = "good_file"


def input_fn(df):
    # Creates a dictionary mapping from each categorical feature column name (k)
    # to the values of that column stored in a tf.SparseTensor.
    categorical_cols = {k: tf.SparseTensor(
        indices=[[i, 0] for i in range(df[k].size)],
        values=df[k].values,
        dense_shape=[df[k].size, 1])
                        for k in CATEGORICAL_COLUMNS}
    # Merges the two dictionaries into one.
    feature_cols = dict(categorical_cols.items())
    # Converts the label column into a constant Tensor.
    label = tf.constant(df[LABEL_COLUMN].values)
    # Returns the feature columns and the label.
    return feature_cols, label


def input_fn_train(df):
    return input_fn(df)


def input_fn_eval(df):
    return input_fn(df)


def create_neural_net(data_file, imports_file, classifier_write_file, hidden_layers, layer_num):
    to_test = range(50000)
    fields = ['imports', 'good_file']
    fields_num = [2, 5]
    df_parser = pd.read_csv(data_file, chunksize=5000, iterator=True, skipinitialspace=True,
                            skiprows=to_test, usecols=fields_num)

    text_encoding = open(imports_file, 'r').read()
    encoder = CountVectorizer()
    encoder.fit(text_encoding.split())
    num_features = len(encoder.get_feature_names())
    print "Features " + str(num_features)
    #print encoder.get_feature_names()
    #imports_column = tf.contrib.layers.sparse_column_with_keys(column_name="imports", keys=encoder.get_feature_names())
    #good_file_column = tf.contrib.layers.sparse_column_with_keys(column_name="good_file", keys=[0, 1])
    #words = tf.contrib.layers.embed_sequence()

    #learner = tf.contrib.learn.DNNClassifier(hidden_units=[3000, 1500, 750],
    #                                        feature_columns=encoder.get_feature_names())
    learner = MLPClassifier(hidden_layer_sizes=(6000, 3000, 1500), verbose=True)
    total_read = 0
    for dataframe in df_parser:
        dataframe.columns = ['imports', 'good_file']
        dataframe_test = dataframe.sample(frac=.1, random_state=40)
        dataframe_train = dataframe.sample(frac=.9, random_state=40)
        data_features = dataframe['good_file']
        data_imports = dataframe['imports']
        data_imports_encoded = encoder.transform(data_imports)
        # data_imports = chunk[chunk.columns[0]]
        # data_features = chunk[chunk.columns[1]]

        # print str(data_imports) + "\n"
        # print str(data_features) + "\n"
        testing_size_nn = .1
        x_train_nn, x_test_nn, y_train_nn, y_test_nn = train_test_split(data_imports_encoded, data_features,
                                                                        test_size=testing_size_nn,
                                                                        random_state=random.randint(1,100))

        #print x_train_nn
        try:
            learner.partial_fit(x_train_nn, y_train_nn, [0., 1.])
            total_read += 5000
            print "Score: for " + str(total_read) + " is " + str(learner.score(x_test_nn, y_test_nn))
        except:
            print "Broke for indicies: " + str(total_read + 5000)

    joblib.dump(learner, classifier_write_file)

'''
    data_final_test = pd.read_csv(data_file, nrows=50000, skipinitialspace=True,
                            usecols=fields_num)
    data_final_test.columns = ['imports', 'good_file']

    data_final_test_imports = data_final_test['imports']
    data_final_test_features = data_final_test['good_file']

    print "NN Score: " + str(learner.score(data_final_test_imports, data_final_test_features))

    neural_score = learner.predict(data_final_test_imports)
    fpr_nn, tpr_nn, thresholds = roc_curve(data_final_test_features, neural_score)
    print "False Positive Rate for NN " + str(fpr_nn)
    print "True Positive Rate for NN " + str(tpr_nn)
    print "Thresholds: " + str(thresholds)
'''




def main():
    parser = argparse.ArgumentParser(description="Neural Network for Blue Sentry")
    parser.add_argument("--data_file", help="Data file from FileIndexer", required=True)
    parser.add_argument("--imports_file", help="The file which holds the imports prased via the ImportsIndexer", required=True)
    parser.add_argument("--classifier_write_file", help="File to write the trained classifier to", required=True)
    parser.add_argument("--hidden_layers", help="File to write the trained classifier to", required=False, default=1000)
    parser.add_argument("--layer_num", help="File to write the trained classifier to", required=False, default=3)


    args = parser.parse_args()

    create_neural_net(args.data_file, args.imports_file, args.classifier_write_file, args.hidden_layers, args.layer_num)

if __name__ == "__main__":
    main()


