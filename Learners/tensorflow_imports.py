import numpy as np
import pandas as pd
import joblib
import argparse
from IPython.display import display
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.neural_network import MLPClassifier
import random
import tensorflow as tf
from tensorflow.contrib.layers.python.layers import encoders

CATEGORICAL_COLUMNS = ["imports"]
LABEL_COLUMN = "good_file"


def input_fn(df):
    # Creates a dictionary mapping from each categorical feature column name (k)
    # to the values of that column stored in a tf.SparseTensor.
    categorical_cols = {k: tf.SparseTensor(
        indices=[[i, 0] for i in range(df[k].size)],
        values=df[k].values,
        dense_shape=[df[k].size, 1])
                        for k in CATEGORICAL_COLUMNS}
    # Merges the two dictionaries into one.
    feature_cols = dict(categorical_cols.items())
    # Converts the label column into a constant Tensor.
    label = tf.constant(df[LABEL_COLUMN].values)
    # Returns the feature columns and the label.
    return feature_cols, label


def input_fn_train(df):
    return input_fn(df)


def input_fn_eval(df):
    return input_fn(df)


def create_neural_net(data_file, imports_file, classifier_write_file, hidden_layers, layer_num):
    to_test = range(50000)
    fields = ['imports', 'good_file']
    fields_num = [2, 5]
    df_parser = pd.read_csv(data_file, chunksize=5000, iterator=True, skipinitialspace=True,
                            skiprows=to_test, usecols=fields_num)

    text_encoding = open(imports_file, 'r').read()
    encoder = CountVectorizer()
    encoder.fit(text_encoding.split())
    num_features = len(encoder.get_feature_names())
    print "Features " + str(num_features)
    num_labels = 2
    X = tf.placeholder(tf.float32, [None, num_features])
    Y = tf.placeholder(tf.float32, [None, num_labels])

    weights = tf.Variable(tf.random_normal([num_features, num_labels], mean=0,
                                           stddev=(np.sqrt(6 / num_features + num_labels + 1)), name="weights"))
    bias = tf.Variable(tf.random_normal([1, num_labels], mean=0, stddev=(np.sqrt(6 / num_features + num_labels + 1)),
                                        name="bias"))
    init_OP = tf.initialize_all_variables()
    apply_weights_OP = tf.matmul(X, weights, name="apply_weights")
    add_bias_OP = tf.add(apply_weights_OP, bias, name="add bias")
    activation_OP = tf.nn.sigmoid(add_bias_OP, name="activation")

    learner = tf.contrib.learn.DNNClassifier(hidden_units=[3000, 1500, 750],
                                            feature_columns=encoder.get_feature_names())
    total_read = 0
    for df in df_parser:
        df.columns = ['imports', 'good_file']



def main():
    parser = argparse.ArgumentParser(description="Neural Network for Blue Sentry")
    parser.add_argument("--data_file", help="Data file from FileIndexer", required=True)
    parser.add_argument("--imports_file", help="The file which holds the imports prased via the ImportsIndexer", required=True)
    parser.add_argument("--classifier_write_file", help="File to write the trained classifier to", required=True)
    parser.add_argument("--hidden_layers", help="File to write the trained classifier to", required=False, default=1000)
    parser.add_argument("--layer_num", help="File to write the trained classifier to", required=False, default=3)


    args = parser.parse_args()

    create_neural_net(args.data_file, args.imports_file, args.classifier_write_file, args.hidden_layers, args.layer_num)

if __name__ == "__main__":
    main()