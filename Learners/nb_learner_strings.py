import numpy as np
import pandas as pd
import joblib
import argparse
from IPython.display import display
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.neural_network import MLPClassifier
import random
from sklearn.naive_bayes import MultinomialNB
import sys

SIZE_OF_SAMPLE = 10000


def create_neural_net(data_file, strings_file, classifier_write_file, hidden_layers, layer_num):
    to_test = range(50000)
    fields = ['strings', 'good_file']
    fields_num = [1, 5]
    df_parser = pd.read_csv(data_file, chunksize=SIZE_OF_SAMPLE, iterator=True, skipinitialspace=True,
                            skiprows=to_test, usecols=fields_num)

    text_encoding = open(strings_file, 'r').read()
    encoder = CountVectorizer()
    encoder.fit(text_encoding.split())
    num_features = len(encoder.get_feature_names())
    print "Features " + str(num_features)

    learner = MultinomialNB()
    total_read = 0
    for df in df_parser:
        df.columns = ['strings', 'good_file']
        data_features = df['good_file']
        data_strings = df['strings']
        data_strings_encoded = encoder.transform(data_strings)

        testing_size_nn = .1
        x_train_nn, x_test_nn, y_train_nn, y_test_nn = train_test_split(data_strings_encoded, data_features,
                                                                        test_size=testing_size_nn,
                                                                        random_state=random.randint(1, 100))

        try:
            learner.partial_fit(x_train_nn, y_train_nn, [0., 1.])
            total_read += SIZE_OF_SAMPLE
            print "Score: for " + str(total_read) + " is " + str(learner.score(x_test_nn, y_test_nn))
        except:
            e = sys.exc_info()[0]
            print "Broke for indicies: " + str(total_read + SIZE_OF_SAMPLE)
            print e

    joblib.dump(learner, classifier_write_file)


def main():
    parser = argparse.ArgumentParser(description="Neural Network for Blue Sentry")
    parser.add_argument("--data_file", help="Data file from FileIndexer", required=True)
    parser.add_argument("--strings_file", help="The file which holds the imports prased via the ImportsIndexer", required=True)
    parser.add_argument("--classifier_write_file", help="File to write the trained classifier to", required=True)
    parser.add_argument("--hidden_layers", help="File to write the trained classifier to", required=False, default=1000)
    parser.add_argument("--layer_num", help="File to write the trained classifier to", required=False, default=3)

    args = parser.parse_args()

    create_neural_net(args.data_file, args.strings_file, args.classifier_write_file, args.hidden_layers, args.layer_num)

if __name__ == "__main__":
    main()


