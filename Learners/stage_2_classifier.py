import numpy as np
import pandas as pd
import joblib
import argparse
import random
import sys

from sklearn import neighbors
from sklearn import linear_model
from IPython.display import display
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.metrics import roc_curve
from sklearn import ensemble
from sklearn.neural_network import MLPClassifier

SIZE_OF_SAMPLE = 10000


def get_code(tree, feature_names):
        left      = tree.tree_.children_left
        right     = tree.tree_.children_right
        threshold = tree.tree_.threshold
        features  = [feature_names[i] for i in tree.tree_.feature]
        value = tree.tree_.value

        def recurse(left, right, threshold, features, node):
                if (threshold[node] != -2):
                        print "if ( " + features[node] + " <= " + str(threshold[node]) + " ) {"
                        if left[node] != -1:
                                recurse (left, right, threshold, features,left[node])
                        print "} else {"
                        if right[node] != -1:
                                recurse (left, right, threshold, features,right[node])
                        print "}"
                else:
                        print "return " + str(value[node])

        recurse(left, right, threshold, features, 0)

def main():

    parser = argparse.ArgumentParser(description="Stage 2 Learner for Blue Sentry")
    parser.add_argument("--data_file", help="Data File from add_data_from_imports_learner", required=True)
    parser.add_argument("--classifier_write_file", help="The file containing the saved classifier", required=True)
    args = parser.parse_args()
    to_test = range(50000)
    data_file = args.data_file
    df_parser = pd.read_csv(data_file, chunksize=SIZE_OF_SAMPLE, iterator=True, skipinitialspace=True,
                            skiprows=to_test)

    #display(data.describe)
    clf = MLPClassifier(hidden_layer_sizes=(1000,1000,1000), verbose=True)
    #clf = linear_model.SGDClassifier() #Bad
    #clf = DecisionTreeClassifier() #Good
    #clf = svm.SVC(cache_size=1000) # Bad / Slow
    #clf = ensemble.RandomForestClassifier() #Better_Dat
    #clf = AdaBoostClassifier() #Meh
    #clf = ensemble.GradientBoostingClassifier(max_depth=15)
    total_read = 0
    for df in df_parser:
        df.columns = ['sha256', 'size', 'signed', 'good_file', 'entropy', 'imports_processed', 'strings_processed']
        df = df.drop(['size'], axis=1)
        data_features = df['good_file']
        df = df.drop(['good_file'], axis=1)
        df = df.drop(['sha256'], axis=1)
        testing_size_nn = .1
        x_train_nn, x_test_nn, y_train_nn, y_test_nn = train_test_split(df, data_features,
                                                                        test_size=testing_size_nn,
                                                                        random_state=random.randint(1, 100))
        #clf.partial_fit(x_train_nn, y_train_nn, [0., 1.])
        try:
            clf.partial_fit(x_train_nn, y_train_nn, [0., 1.])
            total_read += SIZE_OF_SAMPLE
            print "Score: for " + str(total_read) + " is " + str(clf.score(x_test_nn, y_test_nn))
        except:
            e = sys.exc_info()[0]
            print "Broke for indicies: " + str(total_read + SIZE_OF_SAMPLE)
            print e

    joblib.dump(clf, args.classifier_write_file)

if __name__ == "__main__":
    main()

